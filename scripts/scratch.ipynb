{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typet5.utils import *\n",
    "from typet5.static_analysis import UsageAnalysis, PythonProject\n",
    "from typet5.function_dataset import data_project_from_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ManyTypes4Py\"\n",
    "\n",
    "result_paths = {\n",
    "    \"CodeT5\": get_eval_dir(dataset, \"\"),\n",
    "    \"TypeT5\": get_eval_dir(dataset, \"(implicit_imports, new) model-v7--TrainingConfig(drop_env_types=False, add_implicit_rel_imports=True)\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_proj = PythonProject.parse_from_root(Path(\"/home/jiayi/Projects/type4py\"))\n",
    "analysis = UsageAnalysis(\n",
    "    ex_proj, add_implicit_rel_imports=True, add_override_usages=True\n",
    ")\n",
    "pretty_print_dict(analysis.get_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.data import (\n",
    "    create_tokenized_srcsets,\n",
    "    get_tk_dataset_name,\n",
    "    load_tokenized_srcsets,\n",
    "    TypeCheckSettings,\n",
    ")\n",
    "from typet5.tokenized_src import PreprocessArgs\n",
    "\n",
    "pre_args = PreprocessArgs()\n",
    "dataset = \"InferTypes4Py\"\n",
    "sdata_name = get_tk_dataset_name(dataset, pre_args, False)\n",
    "sdata_path = get_dataroot() / \"TokenizedSrcSets\" / sdata_name\n",
    "create_tokenized_srcsets(\n",
    "    dataset,\n",
    "    sdata_path,\n",
    "    func_only=False,\n",
    "    pre_args=pre_args,\n",
    ")\n",
    "tk_dataset = load_tokenized_srcsets(sdata_path)\n",
    "tk_dataset[\"test\"].print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.static_analysis import ProjectPath\n",
    "\n",
    "analysis.user2used[ProjectPath.from_str(\"type4py.type_check/MypyManager._build_tc_cmd\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.utils import *\n",
    "\n",
    "def show_subtokens(ids: list[int]):\n",
    "    tks = [f\"⦗{x}⦘\" for x in DefaultTokenizer.convert_ids_to_tokens(ids)]\n",
    "    print(\" \".join(tks))\n",
    "\n",
    "ex_code = \"\"\"\n",
    "def eval_on_dataset(\n",
    "    model: <extra_id_0>,\n",
    "    data: <extra_id_1>,\n",
    "    window_size: <extra_id_2> = None,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "encoding = DefaultTokenizer.encode(ex_code)\n",
    "print(encoding)\n",
    "show_subtokens(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_output = \"\"\"\n",
    "<extra_id_0>ModelWrapper<extra_id_1>TokenizedSrcSet<extra_id_2>Optional[int]\n",
    "\"\"\"\n",
    "print(decoding := DefaultTokenizer.encode(ex_output))\n",
    "show_subtokens(decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import proj_root\n",
    "from typet5.static_analysis import ProjectPath, UsageAnalysis, PythonProject\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "proj = PythonProject.parse_from_root(proj_root())\n",
    "for caller, callees in UsageAnalysis(proj).user2used.items():\n",
    "    if caller.module == \"typet5.static_analysis\":\n",
    "        print(caller)\n",
    "        for callee in callees:\n",
    "            print(\"\\t\", callee.used, \"\" if callee.is_certain else \"  (maybe)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typet5.tokenized_src import PreprocessArgs, proj_root\n",
    "from typet5.function_dataset import repo_to_tk_srcs, dataset_from_repos\n",
    "\n",
    "srcs = repo_to_tk_srcs(proj_root(), PreprocessArgs(drop_env_types=True))\n",
    "\n",
    "from typet5.data import TokenizedSrcSet, CtxArgs\n",
    "\n",
    "sdata = TokenizedSrcSet(proj_root(), srcs)\n",
    "ctx_args = CtxArgs(1024, 128, 256, 512)\n",
    "cdata = sdata.to_chunks(ctx_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.utils import *\n",
    "\n",
    "print(decode_tokens(cdata.data[\"input_ids\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src in [s for s in srcs if \"static_analysis/PythonFunction\" in str(s.file)][:10]:\n",
    "    print(f\"======= file: {src.file} ========\")\n",
    "    src.print_code(max_lines=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.data import PreprocessArgs, repr_modified_args\n",
    "\n",
    "repr_modified_args(\n",
    "    TrainingConfig(pre_args=PreprocessArgs(imports_in_preamble=False)), flatten=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.static_analysis import cst, PythonModule, compute_module_usages, PythonProject\n",
    "\n",
    "code1 = \"\"\"\n",
    "# root.file1\n",
    "\n",
    "# global function\n",
    "def gf(x):\n",
    "    return x * x\n",
    "\n",
    "# with inner function\n",
    "def gf_with_inner(x):\n",
    "    def inner(y):\n",
    "        return y * y\n",
    "    return inner(x)\n",
    "\n",
    "# class\n",
    "class C:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def foo(self, y):\n",
    "        return self.x + y\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x + 1\n",
    "    \n",
    "\"\"\"\n",
    "code2 = \"\"\"\n",
    "# root.file2\n",
    "from .file1 import gf\n",
    "from root.file1 import gf_with_inner\n",
    "import root.file1\n",
    "import root.file1 as f1\n",
    "\n",
    "def usage1(x):\n",
    "    gf(x) + root.file1.C(5)\n",
    "    foo(5)\n",
    "\n",
    "def usage2(x):\n",
    "    def inner():\n",
    "        1 + gf_with_inner(x)\n",
    "    return inner()\n",
    "\n",
    "def usage_method1(x):\n",
    "    x = f1.C(5)\n",
    "    1 + x.foo(3)\n",
    "\n",
    "def usage_method2(x):\n",
    "    (1 + f1.C(5)).foo(3)\n",
    "\n",
    "def usage_local():\n",
    "    usage1(3)\n",
    "    UsageClass(4)\n",
    "\n",
    "@f1.C(1)\n",
    "def usage_dec():\n",
    "    pass\n",
    "\n",
    "class UsageClass:\n",
    "    def __init__(self, x):\n",
    "        self.x = gf_with_inner(x)\n",
    "        self.y = self.foo(5)\n",
    "\n",
    "    def foo(self, y):\n",
    "        return usage_local(f1.gf(y))\n",
    "\n",
    "    @staticmethod\n",
    "    def s_method(x):\n",
    "        return x\n",
    "\n",
    "class SubClass(UsageClass):\n",
    "    def use(self):\n",
    "        self.foo(5)\n",
    "        f1.C.s_method(5)\n",
    "\"\"\"\n",
    "\n",
    "code3 = \"\"\"\n",
    "# root.file3\n",
    "from .file1 import *\n",
    "\n",
    "def usage1(x):\n",
    "    gf(5)\n",
    "    C(5)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "project = PythonProject.from_modules(\n",
    "    [\n",
    "        PythonModule.from_cst(cst.parse_module(code1), \"root.file1\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code2), \"root.file2\"),\n",
    "        PythonModule.from_cst(cst.parse_module(code3), \"root.file3\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "for u in compute_module_usages(project.modules[\"root.file3\"]):\n",
    "    print(str(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.static_analysis import build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.static_analysis import compute_module_usages\n",
    "\n",
    "compute_module_usages(project.modules[\"root.file3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.static_analysis import UsageAnalysis, build_project_namespaces\n",
    "\n",
    "build_project_namespaces(project)\n",
    "\n",
    "analysis = UsageAnalysis(project)\n",
    "analysis.caller2callees[ProjectPath(\"root.file2\", \"SubClass.use\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from typet5.tokenized_src import TokenizedSrc, PreprocessArgs\n",
    "from typet5.utils import Path, decode_tokens\n",
    "\n",
    "ex_code = '''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "@wraps(function)\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "print(decode_tokens(ex_src.tokenized_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.data import src_to_chunks_, CtxArgs, PreprocessArgs\n",
    "from ipywidgets import interactive\n",
    "\n",
    "pre_args = PreprocessArgs(stub_in_preamble=True)\n",
    "ex_src = TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\"), pre_args)\n",
    "\n",
    "\n",
    "def print_code(\n",
    "    preamble: int,\n",
    "    left: int,\n",
    "    right: int,\n",
    "    ctx_size: int,\n",
    "    max_labels: int,\n",
    "    chunk_id: int,\n",
    "    inline_prev: bool,\n",
    "):\n",
    "    chunks = []\n",
    "    args = CtxArgs(\n",
    "        ctx_size,\n",
    "        preamble,\n",
    "        left,\n",
    "        right,\n",
    "        max_labels=max_labels,\n",
    "        inline_prev_gold=inline_prev,\n",
    "    )\n",
    "    src_to_chunks_(chunks, [], ex_src, (0, len(ex_src.types)), args)\n",
    "    print(decode_tokens(chunks[chunk_id][\"input_ids\"]))\n",
    "\n",
    "\n",
    "interactive(\n",
    "    print_code,\n",
    "    preamble=(1, 100),\n",
    "    left=(1, 200),\n",
    "    right=(1, 100),\n",
    "    ctx_size=(1, 500),\n",
    "    max_labels=(1, 10),\n",
    "    chunk_id=(0, 1),\n",
    "    inline_prev=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from typet5.data import GitRepo, ModuleRemapUnpickler\n",
    "from typet5.type_env import (\n",
    "    AnnotPath,\n",
    "    MypyChecker,\n",
    "    SelectAnnotations,\n",
    "    TypeInfAction,\n",
    "    TypeInfEnv,\n",
    "    TypeInfState,\n",
    "    mypy_checker,\n",
    ")\n",
    "from typet5.utils import cst, proj_root, read_file, seq_flatten, tqdm, write_file\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = Path(os.getenv(\"datadir\"))\n",
    "repos_dir = datadir / \"SPOT-data/repos\"\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "rename_module = lambda n: \"typet5.data\" if n == \"typet5.data_prepare\" else n\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    useful_repos: list[GitRepo] = ModuleRemapUnpickler(f, rename_module).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained model and tokenizer\n",
    "from typet5.utils import get_data_dir\n",
    "\n",
    "model_dir = \"Salesforce/codet5-base\"\n",
    "# model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-no_margin/\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.models.t5 import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_dir\n",
    ").to(device)\n",
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.data import mask_type_annots, output_ids_as_types, tokenize_masked\n",
    "\n",
    "test_code = \"\"\"\n",
    "@dataclass\n",
    "class GitRepo:\n",
    "    author: str\n",
    "    name: str\n",
    "    url: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "\n",
    "    def authorname(self):\n",
    "        return self.author + \"__\" + self.name\n",
    "\n",
    "    def repo_dir(self, repos_dir: Path) -> Path:\n",
    "        return repos_dir / \"downloaded\" / self.authorname()\n",
    "\n",
    "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_model(code: str, num_beams=16):\n",
    "    masked = mask_type_annots((Path(\"no_source\"), code))\n",
    "    tks = tokenize_masked(masked, tokenizer, device)\n",
    "    input_ids = tks[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model.forward(**tks).loss\n",
    "        dec = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=num_beams,\n",
    "            # do_sample=True,\n",
    "        )[0]\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"labels\": output_ids_as_types(tks[\"labels\"][0], tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "        \"input_ids\": input_ids[0],\n",
    "        \"output_ids\": dec,\n",
    "        \"annots_info\": masked[\"annots_info\"],\n",
    "    }\n",
    "\n",
    "\n",
    "result = run_model(test_code, num_beams=10)\n",
    "result[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import PythonType\n",
    "from typet5.type_env import apply_annotations\n",
    "\n",
    "\n",
    "def type_to_annot(ty: PythonType) -> str:\n",
    "    return cst.Annotation(cst.parse_expression(str(ty)))\n",
    "\n",
    "\n",
    "def run_aug_model(src: Path, cwd: Path):\n",
    "    result = run_model(read_file(src), num_beams=10)\n",
    "    pred_annots = {\n",
    "        info.path: type_to_annot(t)\n",
    "        for info, t in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    m1 = apply_annotations(cst.parse_module(read_file(src)), pred_annots)\n",
    "    write_file(src, m1.code)\n",
    "    checker_r = MypyChecker.check_project(src, cwd)\n",
    "    pos_to_preds = {\n",
    "        info.annot_range: str(ty)\n",
    "        for info, ty in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    return {\n",
    "        \"model_result\": result,\n",
    "        \"module\": m1,\n",
    "        \"checker_feedback\": checker_r,\n",
    "        \"pos_to_preds\": pos_to_preds,\n",
    "    }\n",
    "\n",
    "\n",
    "aug_r = run_aug_model(inference_dir / \"env_code_2.py\", inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typet5.utils import patch_code_with_extra\n",
    "\n",
    "print(\"---- predicted types ----\")\n",
    "print(aug_r[\"model_result\"][\"predicted_types\"])\n",
    "print(\"---- model output ----\")\n",
    "print(tokenizer.decode(aug_r[\"model_result\"][\"output_ids\"], skip_special_tokens=False))\n",
    "print(\"---- checker_feedback ----\")\n",
    "print(aug_r[\"checker_feedback\"].output_str)\n",
    "\n",
    "print(\"---- new input ----\")\n",
    "new_input = patch_code_with_extra(\n",
    "    aug_r[\"module\"].code,\n",
    "    aug_r[\"pos_to_preds\"],\n",
    "    aug_r[\"checker_feedback\"].error_dict[\"env_code_2.py\"],\n",
    ")\n",
    "print(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from typet5.utils import Path, run_long_task, DefaultTokenizer, not_none, CountedAcc\n",
    "from spot import proj_root\n",
    "from typet5.function_dataset import guess_src_root\n",
    "\n",
    "datadir = Path(not_none(os.getenv(\"datadir\")))\n",
    "repos_dir = datadir / \"SPOT-data/repos/\"\n",
    "\n",
    "repos_split_path = proj_root() / \"data/repos_split.pkl\"\n",
    "with repos_split_path.open(\"rb\") as f:\n",
    "    repos_split = pickle.load(f)\n",
    "\n",
    "root_is_src = list[bool]()\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd = repo.repo_dir(repos_dir)\n",
    "    root_is_src.append(guess_src_root(rd).name == \"src\")\n",
    "\n",
    "CountedAcc(sum(root_is_src), len(root_is_src))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_in_root = 0\n",
    "package_in_root = 0\n",
    "setup_in_root = 0\n",
    "n_proj = 0\n",
    "\n",
    "weird_repos = []\n",
    "setup_files = []\n",
    "\n",
    "for repo in repos_split[\"train\"]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    n_proj += 1\n",
    "    files = list(rd.iterdir())\n",
    "    if rd / \"src\" in files:\n",
    "        src_in_root += 1\n",
    "    elif rd / (pname := rd.name.split(\"__\")[-1]) in files:\n",
    "        package_in_root += 1\n",
    "    elif rd / \"setup.cfg\" in files:\n",
    "        setup_in_root += 1\n",
    "        setup_files.append(rd / \"setup.cfg\")\n",
    "    else:\n",
    "        weird_repos.append(repo)\n",
    "\n",
    "print(\"n_projects:\", n_proj)\n",
    "print(\"src_in_root:\", src_in_root)\n",
    "print(\"package_in_root:\", package_in_root)\n",
    "print(\"setup_in_root:\", setup_in_root)\n",
    "print(\"weird_repos:\", len(weird_repos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in weird_repos[:10]:\n",
    "    rd: Path = repo.repo_dir(repos_dir)\n",
    "    print(\"Repo:\", rd.relative_to(repos_dir))\n",
    "    for f in rd.iterdir():\n",
    "        print(f.relative_to(rd))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
