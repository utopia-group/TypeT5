{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========TokenizedSrc========\n",
      "file:test_file\n",
      "repo:test_repo\n",
      "--------Preamble--------\n",
      "import os; \n",
      "from sys import argv, exit\n",
      "import spot;\n",
      "\n",
      "--------Main Code--------\n",
      "def catch_permission_denied(function):\n",
      "    import some.inner.imports\n",
      "    @wraps(function)\n",
      "    def decorated(x: str, y: int) -> str:\n",
      "        try:\n",
      "            return function(*args, **kwargs)\n",
      "\n",
      "        except InsufficientPrivilege as error:\n",
      "            LOG.error(\"Forbidden: %s\", error) \n",
      "            raise Forbidden()\n",
      "\n",
      "    return decorated\n",
      "\n",
      "========End of TokenizedSrc========\n"
     ]
    }
   ],
   "source": [
    "import libcst as cst\n",
    "\n",
    "from spot.tokenized_src import TokenizedSrc\n",
    "from spot.utils import Path\n",
    "\n",
    "ex_code='''# document comment 1\n",
    "  # document comment 2\n",
    "\"\"\"String document commnet\"\"\"\n",
    "import os; import spot;\n",
    "from sys import argv, exit\n",
    "# after import\n",
    "def catch_permission_denied(function):\n",
    "    import some.inner.imports\n",
    "    \"\"\"\n",
    "    Decorator to catch :class:`psycopg2.ProgrammingError` exceptions with the\n",
    "    ``INSUFFICIENT_PRIVILEGE`` error code and rethrow them as\n",
    "    :class:`~werkzeug.exceptions.Forbidden` exceptions instead.\n",
    "    \"\"\"\n",
    "    @wraps(function)\n",
    "    def decorated(x: str, y: int) -> str:\n",
    "        try:\n",
    "            # comment 1\n",
    "            # comment 1 cont\n",
    "            return function(*args, **kwargs)\n",
    "\n",
    "        except InsufficientPrivilege as error:\n",
    "            LOG.error(\"Forbidden: %s\", error) # comment 2\n",
    "            raise Forbidden()\n",
    "\n",
    "    return decorated\n",
    "'''\n",
    "print(TokenizedSrc.parse(ex_code, Path(\"test_file\"), Path(\"test_repo\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_files_too_wide: 0\n",
      "too_wide_ratio: 0\n",
      "drop_comments: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9ec0602500463a9738b805a1e25702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mask_type_annots:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c74cb4213ae446b9628214f86277bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dict_to_tokenized_src:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spot.data import SrcDataset\n",
    "from spot.utils import DefaultTokenizer, proj_root\n",
    "\n",
    "simple_dataset = SrcDataset.from_repos(\n",
    "    proj_root() / \"data\",\n",
    "    [proj_root() / \"data/code\"],\n",
    "    DefaultTokenizer,\n",
    "    drop_comments=True,\n",
    "    max_workers=10,\n",
    "    label_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca56a5acd8c2432dbdb94fe691dcca3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "type_check_src:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedbacks:\n",
      "MypyFeedback(position=CodePosition(line=14, column=12), message='Incompatible return value type (got \"str\", expected \"int\") ', error_code='return-value')\n",
      "======= New code =======\n",
      " 1|  from typing import Any # SPOT\n",
      " 2|  from typing import Any\n",
      " 3|  \n",
      " 4|  def fib(n: str) -> Any:\n",
      " 5|      if n == 0:\n",
      " 6|          return 0\n",
      " 7|      elif n == 1:\n",
      " 8|          return 1\n",
      " 9|      else:\n",
      "10|          return fib(n-1) + fib(n-2)\n",
      "11|  \n",
      "12|  def t_add(x: str, y: str) -> int:\n",
      "13|      r = x + y\n",
      "14|      return r\n",
      "15|  \n",
      "16|  x: int = fib(3)\n",
      "17|  bad_y: str = 1\n",
      "Feedbacks:\n",
      "MypyFeedback(position=CodePosition(line=5, column=5), message='Argument 1 to \"fib\" has incompatible type \"int\"; expected \"str\" ', error_code='arg-type')\n",
      "======= New code =======\n",
      "1|  from typing import Any # SPOT\n",
      "2|  from bad_code_1 import fib\n",
      "3|  \n",
      "4|  i: int = 4\n",
      "5|  fib(i)\n",
      "6|  \n",
      "Feedbacks:\n",
      "MypyFeedback(position=CodePosition(line=4, column=10), message='Incompatible types in assignment (expression has type \"int\", variable has type \"str\") ', error_code='assignment')\n",
      "======= New code =======\n",
      "1|  from typing import Any # SPOT\n",
      "2|  from dummy.dummy_1 import f_int\n",
      "3|  \n",
      "4|  s: str = f_int(2)\n",
      "5|  \n"
     ]
    }
   ],
   "source": [
    "file2preds = {\n",
    "    (proj_root() / \"data/code/bad_code_1.py\"): {\n",
    "        1: \"str\",\n",
    "        2: \"str\",\n",
    "    },\n",
    "    (proj_root() / \"data/code/bad_code_2.py\"): {\n",
    "        0: \"int\",\n",
    "    },\n",
    "    (proj_root() / \"data/code/dummy/dummy_2.py\"): {\n",
    "        0: \"str\",\n",
    "    }\n",
    "}\n",
    "fdbks = simple_dataset._get_type_checker_feedback_iso(\n",
    "    file2preds,\n",
    "    max_workers=20,\n",
    ")\n",
    "for f in fdbks:\n",
    "    f.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedbacks:\n",
      "======= New code =======\n",
      "from typing import Any # SPOT\n",
      "from bad_code_1 import fib\n",
      "\n",
      "i: int = 4\n",
      "fib(i)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.data import Path, type_check_src, type_check_src_in_project\n",
    "\n",
    "src_to_check = simple_dataset.get_src_by_file(Path(\"bad_code_2.py\"))\n",
    "type_check_src(src_to_check, {0: \"int\"}).pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedbacks:\n",
      "MypyFeedback(position=CodePosition(line=5, column=5), message='Argument 1 to \"fib\" has incompatible type \"int\"; expected \"str\" ', error_code='arg-type')\n",
      "======= New code =======\n",
      "from typing import Any # SPOT\n",
      "from bad_code_1 import fib\n",
      "\n",
      "i: int = 4\n",
      "fib(i)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.data import type_check_src, type_check_src_in_project\n",
    "import shutil\n",
    "\n",
    "src_to_check = simple_dataset.get_src_by_file(Path(\"bad_code_2.py\"))\n",
    "temp_dir = (proj_root() / \"mypy_temp/test_dir\")\n",
    "shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "type_check_src_in_project(\n",
    "    src_to_check,\n",
    "    {0: \"int\"},\n",
    "    project_files=(proj_root() / \"data/code\").glob(\"**/*.py\"),\n",
    "    project_root=(proj_root() / \"data/code\"),\n",
    "    temp_dir=temp_dir,\n",
    ").pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import logging\n",
      "import pickle\n",
      "import random\n",
      "import shutil\n",
      "import subprocess\n",
      "import warnings\n",
      "from copy import copy\n",
      "from dataclasses import dataclass\n",
      "from datetime import datetime\n",
      "from typing import *\n",
      "\n",
      "import dateparser\n",
      "from datasets import Dataset\n",
      "\n",
      "from spot.type_env import (\n",
      "    AnnotCat,\n",
      "    AnnotInfo,\n",
      "    AnnotPath,\n",
      "    MypyChecker,\n",
      "    PythonType,\n",
      "    apply_annotations,\n",
      "    collect_annots_info,\n",
      "    collect_user_annotations,\n",
      "    normalize_type,\n",
      "    parse_type_expr,\n",
      "    parse_type_from_ast,\n",
      ")\n",
      "from spot.utils import *\n",
      "\n",
      "warnings.filterwarnings(\n",
      "    \"ignore\",\n",
      "    message=\"The localize method is no longer necessary, as this time zone supports the fold attribute\",\n",
      ")\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class GitRepo:\n",
      "    author: str\n",
      "    name: str\n",
      "    url: str\n",
      "    stars: int\n",
      "    forks: int\n",
      "    lines_of_code: Optional[int] = None\n",
      "    last_update: Optional[datetime] = None\n",
      "    n_type_annots: Optional[int] = None\n",
      "    n_type_places: Optional[int] = None\n",
      "\n",
      "    def authorname(self):\n",
      "        return self.author + \"__\" + self.name\n",
      "\n",
      "    def repo_dir(self, repos_dir: Path) -> Path:\n",
      "        return repos_dir / \"downloaded\" / self.authorname()\n",
      "\n",
      "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
      "        subprocess.run(\n",
      "            [\"git\", \"clone\", \"--depth\", \"1\", self.url, self.authorname()],\n",
      "            cwd=(repos_dir / \"downloading\"),\n",
      "            timeout=timeout,\n",
      "            capture_output=True,\n",
      "        )\n",
      "        if not (repos_dir / \"downloading\" / self.authorname()).is_dir():\n",
      "            return False\n",
      "        subprocess.run(\n",
      "            [\"mv\", self.authorname(), (repos_dir / \"downloaded\")],\n",
      "            cwd=(repos_dir / \"downloading\"),\n",
      "            capture_output=True,\n",
      "        )\n",
      "        return True\n",
      "\n",
      "    def read_last_update(self, repos_dir):\n",
      "        d = self.repo_dir(repos_dir)\n",
      "        s = subprocess.run(\n",
      "            [\"git\", \"log\", \"-1\", \"--format=%cd\"], cwd=d, capture_output=True, text=True\n",
      "        ).stdout\n",
      "        lu = dateparser.parse(s.split(\"+\")[0])\n",
      "        assert lu is not None\n",
      "        self.last_update = lu.replace(tzinfo=None)\n",
      "        return self.last_update\n",
      "\n",
      "    def src_files(self, repos_dir):\n",
      "        for fpath in self.repo_dir(repos_dir).glob(\"**/*.py\"):\n",
      "            yield (fpath, read_file(fpath))\n",
      "\n",
      "    def count_lines_of_code(self, repos_dir):\n",
      "        n_lines = 0\n",
      "        for src in self.repo_dir(repos_dir).glob(\"**/*.py\"):\n",
      "            with open(src, \"r\") as fp:\n",
      "                n_lines += sum(1 for line in fp if line.rstrip())\n",
      "        self.lines_of_code = n_lines\n",
      "        return n_lines\n",
      "\n",
      "    def collect_annotations(\n",
      "        self, repos_dir, silent=True\n",
      "    ) -> dict[Path, dict[AnnotPath, tuple[Optional[PythonType], AnnotCat]]]:\n",
      "        n_paths, n_annots = 0, 0\n",
      "        file_to_annots = dict[\n",
      "            Path, dict[AnnotPath, tuple[Optional[PythonType], AnnotCat]]\n",
      "        ]()\n",
      "        for src in self.repo_dir(repos_dir).glob(\"**/*.py\"):\n",
      "            rpath = src.relative_to(self.repo_dir(repos_dir))\n",
      "            m = cst.parse_module(read_file(src))\n",
      "            paths = collect_annots_info(m)\n",
      "            path_to_cat = {pinfo.path: pinfo.cat for pinfo in paths}\n",
      "            n_paths += len(paths)\n",
      "            annots = (info for info in paths if info.annot is not None)\n",
      "            n_annots += sum(1 for _ in annots)\n",
      "            file_to_annots[rpath] = {\n",
      "                (k := info.path): (\n",
      "                    parse_type_expr(\n",
      "                        m, cast(cst.Annotation, info.annot).annotation, silent\n",
      "                    ),\n",
      "                    path_to_cat[k],\n",
      "                )\n",
      "                for info in annots\n",
      "            }\n",
      "        self.n_type_annots = n_annots\n",
      "        self.n_type_places = n_paths\n",
      "        return file_to_annots\n",
      "\n",
      "    def revert_changes(self, repos_dir):\n",
      "        rd = self.repo_dir(repos_dir)\n",
      "        result = subprocess.run(\n",
      "            [\"git\", \"diff\", \"--name-only\"], cwd=rd, capture_output=True, text=True\n",
      "        )\n",
      "        if result.returncode == 0 and result.stdout.strip() != \"\":\n",
      "            print(\"Reverting changes in\", rd)\n",
      "            subprocess.run(\n",
      "                [\"git\", \"checkout\", \".\"],\n",
      "                cwd=rd,\n",
      "            )\n",
      "\n",
      "    @staticmethod\n",
      "    def from_json(json):\n",
      "        return GitRepo(\n",
      "            author=json[\"author\"],\n",
      "            name=json[\"repo\"],\n",
      "            url=json[\"repoUrl\"],\n",
      "            stars=json[\"stars\"],\n",
      "            forks=json[\"forks\"],\n",
      "        )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class TokenizedSrc:\n",
      "\n",
      "    file: Path\n",
      "    repo: Path\n",
      "    types: list[PythonType]\n",
      "    types_pos: list[int]  \n",
      "    types_str: list[str]\n",
      "    types_tks: list[list[int]]\n",
      "    types_info: list[AnnotInfo]\n",
      "    origin_code: str\n",
      "    tokenized_code: list[int]  \n",
      "\n",
      "\n",
      "class _TokenizedSrcHelper:\n",
      "    tokenizer: TokenizerSPOT\n",
      "\n",
      "    def __init__(self, tokenizer: TokenizerSPOT):\n",
      "        _turn_off_tokenizer_warning(tokenizer)\n",
      "        self.tokenizer = tokenizer\n",
      "\n",
      "    def dict_to_tokenized_src(self, d: dict) -> TokenizedSrc:\n",
      "        r = TokenizedSrc(\n",
      "            file=d[\"file\"],\n",
      "            repo=d[\"repo\"],\n",
      "            origin_code=d[\"original_code\"],\n",
      "            tokenized_code=list[int](),\n",
      "            types=list[PythonType](),\n",
      "            types_pos=list[int](),\n",
      "            types_str=list[str](),\n",
      "            types_info=list[AnnotInfo](),\n",
      "            types_tks=list[list[int]](),\n",
      "        )\n",
      "\n",
      "        match d:\n",
      "            case {\n",
      "                \"code_segs\": segs,\n",
      "                \"types\": types,\n",
      "                \"types_str\": types_str,\n",
      "                \"annots_info\": annots_info,\n",
      "                \"is_label\": is_label,\n",
      "            }:\n",
      "                assert len(segs) == len(types) + 1\n",
      "            case _:\n",
      "                raise ValueError(f\"Invalid dict with keys: {d.keys()}\")\n",
      "\n",
      "        tkn = self.tokenizer\n",
      "        bos_id = not_none(tkn.bos_token_id)\n",
      "        eos_id = not_none(tkn.eos_token_id)\n",
      "        mask_id = not_none(tkn.mask_token_id)\n",
      "        all_tks = r.tokenized_code\n",
      "        all_tks.append(bos_id)\n",
      "        for i in range(len(types)):\n",
      "            all_tks.extend(tkn.encode(segs[i], add_special_tokens=False))\n",
      "            if is_label is None or is_label[i]:\n",
      "                r.types_pos.append(len(all_tks))\n",
      "                r.types.append(types[i])\n",
      "                r.types_tks.append(tkn.encode(str(types[i]), add_special_tokens=False))\n",
      "                r.types_str.append(types_str[i])\n",
      "                r.types_info.append(annots_info[i])\n",
      "                all_tks.append(mask_id)\n",
      "            else:\n",
      "                all_tks.extend(tkn.encode(types_str[i], add_special_tokens=False))\n",
      "        all_tks.extend(tkn.encode(segs[-1], add_special_tokens=False))\n",
      "        all_tks.append(eos_id)\n",
      "\n",
      "        return r\n",
      "\n",
      "    def feedbacks_to_tokenized_src(\n",
      "        self,\n",
      "        src: TokenizedSrc,\n",
      "        current_code: str,\n",
      "        feedbacks: dict[CodePosition, str],\n",
      "    ) -> TokenizedSrc:\n",
      "        try:\n",
      "            m = cst.parse_module(current_code)\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(\n",
      "                f\"Failed to parse file: '{src.file}' with content:\\n{current_code}\"\n",
      "            ) from e\n",
      "        m_code = m.code\n",
      "        assert m_code == current_code, \"Code 1:\\n<<{}>>\\nCode 2:\\n<<{}>>\".format(\n",
      "            current_code, m_code\n",
      "        )\n",
      "        current_annots, _ = collect_user_annotations(m)\n",
      "        preds_map = dict[CodeRange, str]()\n",
      "        types = list[PythonType]()\n",
      "        types_str = list[str]()\n",
      "        annots_info = list[AnnotInfo]()\n",
      "        path2label_id = {info.path: i for i, info in enumerate(src.types_info)}\n",
      "\n",
      "        for a in current_annots:\n",
      "            if a.path in path2label_id:\n",
      "                assert (range := a.annot_range) is not None\n",
      "                assert (annot := a.annot) is not None\n",
      "                preds_map[range] = m.code_for_node(annot.annotation)\n",
      "                li = path2label_id[a.path]\n",
      "                types.append(src.types[li])\n",
      "                types_str.append(src.types_str[li])\n",
      "                annots_info.append(a)\n",
      "        new_code = patch_code_with_extra(current_code, preds_map, feedbacks)\n",
      "        code_segs = new_code.split(SpecialNames.TypeMask)\n",
      "        assert len(code_segs) == len(types) + 1, f\"{len(code_segs)} != {len(types)} + 1\"\n",
      "\n",
      "        d = {\n",
      "            \"file\": src.file,\n",
      "            \"repo\": src.repo,\n",
      "            \"original_code\": new_code,\n",
      "            \"code_segs\": code_segs,\n",
      "            \"types\": types,\n",
      "            \"types_str\": types_str,\n",
      "            \"annots_info\": annots_info,\n",
      "            \"is_label\": None,\n",
      "        }\n",
      "        return self.dict_to_tokenized_src(d)\n",
      "\n",
      "\n",
      "def chunk_srcs(\n",
      "    repos_root: Path,\n",
      "    srcs: Sequence[TokenizedSrc],\n",
      "    tokenizer: TokenizerSPOT,\n",
      "    ctx_args: \"CtxArgs\",\n",
      "    max_workers: int,\n",
      "    tqdm_args: dict,\n",
      ") -> \"ChunkedDataset\":\n",
      "\n",
      "    all_tks = list[int | tuple]()\n",
      "\n",
      "    for src_id, src in enumerate(srcs):\n",
      "        offset = len(all_tks)\n",
      "        all_tks.extend(src.tokenized_code)\n",
      "        for i in range(len(src.types)):\n",
      "            type_tuple = (src.types[i], src.types_info[i], src.types_tks[i], src_id)\n",
      "            label_pos = offset + src.types_pos[i]\n",
      "            all_tks[label_pos] = type_tuple\n",
      "\n",
      "    helper = _ChunkingHelper(tokenizer, ctx_args)\n",
      "    ctx_size, ctx_margin = ctx_args.ctx_size, ctx_args.ctx_margin\n",
      "    stride = ctx_size - 2 * ctx_margin\n",
      "\n",
      "    chunk_tks = [all_tks[i : i + ctx_size] for i in range(0, len(all_tks), stride)]\n",
      "    chunk_outputs = process_map(\n",
      "        helper.process_chunk,\n",
      "        chunk_tks,\n",
      "        desc=\"processing chunks\",\n",
      "        max_workers=max_workers,\n",
      "        chunksize=max(1, len(chunk_tks) // (8 * max_workers)),\n",
      "        **tqdm_args,\n",
      "    )\n",
      "\n",
      "    chunks: dict[str, list] = {\n",
      "        \"input_ids\": [],\n",
      "        \"labels\": [],\n",
      "    }\n",
      "    chunks_info: list[SrcChunkInfo] = []\n",
      "\n",
      "    for chunk in chunk_outputs:\n",
      "        if chunk is None:\n",
      "            continue\n",
      "        chunks[\"input_ids\"].append(chunk[\"input_ids\"])\n",
      "        chunks[\"labels\"].append(chunk[\"labels\"])\n",
      "        meta = chunk[\"meta\"]\n",
      "        chunks_info.append(meta)\n",
      "\n",
      "    files = [(repos_root / s.file).resolve() for s in srcs]\n",
      "    return ChunkedDataset(\n",
      "        data=Dataset.from_dict(chunks),\n",
      "        chunks_info=chunks_info,\n",
      "        files=files,\n",
      "        file2src={f: s.origin_code for f, s in zip(files, srcs)},\n",
      "        file2repo={f: (repos_root / s.repo).resolve() for f, s in zip(files, srcs)},\n",
      "    )\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class SrcDataset:\n",
      "    repos_root: Path\n",
      "    all_srcs: list[TokenizedSrc] = field(default_factory=list)\n",
      "    extra_stats: dict = field(default_factory=dict)\n",
      "\n",
      "    def repos2srcs(self):\n",
      "        r = groupby(self.all_srcs, lambda s: s.repo)\n",
      "        for srcs in r.values():\n",
      "            srcs.sort(key=lambda s: s.file)\n",
      "        return r\n",
      "\n",
      "    def srcs_with_labels(self):\n",
      "        return [s for s in self.all_srcs if len(s.types) > 0]\n",
      "\n",
      "    def add_stats(self, stats: dict, should_print=True):\n",
      "        if should_print:\n",
      "            pretty_print_dict(stats)\n",
      "        self.extra_stats.update(stats)\n",
      "\n",
      "    def to_chunks(\n",
      "        self,\n",
      "        tokenizer: TokenizerSPOT,\n",
      "        ctx_args: \"CtxArgs\",\n",
      "        max_workers: int,\n",
      "        tqdm_args: dict = {},\n",
      "    ) -> \"ChunkedDataset\":\n",
      "        srcs = self.all_srcs\n",
      "        chunks = chunk_srcs(\n",
      "            self.repos_root,\n",
      "            srcs,\n",
      "            tokenizer,\n",
      "            ctx_args,\n",
      "            max_workers=max_workers,\n",
      "            tqdm_args=tqdm_args,\n",
      "        )\n",
      "        chunks.verify_labels(self, tokenizer)\n",
      "        return chunks\n",
      "\n",
      "    def file2src(self):\n",
      "        return {(self.repos_root / s.file).resolve(): s for s in self.all_srcs}\n",
      "\n",
      "    def stats(self) -> dict[str, Any]:\n",
      "        num_repos = len(set(s.repo for s in self.all_srcs))\n",
      "        useful_srcs = self.all_srcs\n",
      "        num_files = len(useful_srcs)\n",
      "        num_lines = sum(len(s.origin_code.split(\"\\n\")) for s in useful_srcs)\n",
      "        tokens_per_file = [len(s.tokenized_code) for s in useful_srcs]\n",
      "        target_tks_per_file = [\n",
      "            sum(len(tks) + 1 for tks in s.types_tks) for s in useful_srcs\n",
      "        ]\n",
      "        basic_stats = {\n",
      "            \"num_repos\": num_repos,\n",
      "            \"num_files\": num_files,\n",
      "            \"num_lines\": num_lines,\n",
      "            \"tokens_per_file\": scalar_stats(tokens_per_file),\n",
      "            \"target_tks_per_file\": scalar_stats(target_tks_per_file),\n",
      "        }\n",
      "        basic_stats.update(self.extra_stats)\n",
      "        return basic_stats\n",
      "\n",
      "    def print_stats(self):\n",
      "        pretty_print_dict(self.stats())\n",
      "\n",
      "    def add_type_checker_feedback(\n",
      "        self,\n",
      "        tokenizer: TokenizerSPOT,\n",
      "        file2preds: dict[Path, dict[int, str]],\n",
      "        max_workers: int,\n",
      "        tqdm_args: dict,\n",
      "        mypy_path: Optional[Path] = None,\n",
      "    ) -> \"SrcDataset\":\n",
      "\n",
      "        file2src = self.file2src()\n",
      "\n",
      "        src_list = [file2src[f.resolve()] for f in file2preds]\n",
      "        chunksize = max(1, len(src_list) // (8 * max_workers))\n",
      "\n",
      "        try:\n",
      "            check_rs = process_map(\n",
      "                type_check_src,\n",
      "                src_list,\n",
      "                list(file2preds.values()),\n",
      "                [mypy_path for _ in src_list],\n",
      "                max_workers=max_workers,\n",
      "                desc=\"type_check_src\",\n",
      "                chunksize=chunksize,\n",
      "                **tqdm_args,\n",
      "            )\n",
      "        finally:\n",
      "            MypyChecker.clear_temp_cache()\n",
      "        n_checked = 0\n",
      "        code_list = list[str]()\n",
      "        feedback_list = list[dict]()\n",
      "        n_error_list = list[int]()\n",
      "        for i in range(len(src_list)):\n",
      "            errors, new_code = check_rs[i]\n",
      "            if isinstance(errors, str):\n",
      "                errors = dict()\n",
      "            else:\n",
      "                n_checked += 1\n",
      "            code_list.append(new_code)\n",
      "            feedback_list.append(errors)\n",
      "            n_error_list.append(len(errors))\n",
      "        result = SrcDataset(self.repos_root)\n",
      "        silent = tqdm_args.get(\"disable\", False)\n",
      "        result.add_stats(\n",
      "            {\n",
      "                \"num_type_checked\": n_checked,\n",
      "                \"errors_per_file\": scalar_stats(n_error_list),\n",
      "            },\n",
      "            not silent,\n",
      "        )\n",
      "\n",
      "        helper = _TokenizedSrcHelper(tokenizer)\n",
      "        new_srcs = process_map(\n",
      "            helper.feedbacks_to_tokenized_src,\n",
      "            src_list,\n",
      "            code_list,\n",
      "            feedback_list,\n",
      "            max_workers=max_workers,\n",
      "            desc=\"feedbacks_to_tokenized_src\",\n",
      "            chunksize=chunksize,\n",
      "            **tqdm_args,\n",
      "        )\n",
      "        result.all_srcs = new_srcs\n",
      "        return result\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"SrcDataset(root='{self.repos_root}', n_repos={len(self.repos2srcs())}, n_labeled_files={len(self.all_srcs)})\"\n",
      "\n",
      "    @staticmethod\n",
      "    def from_repos(\n",
      "        repos_root: Path,\n",
      "        repos_paths: Iterable[Path],\n",
      "        tokenizer: TokenizerSPOT,\n",
      "        max_workers: int,\n",
      "        label_ratio: float = 0.5,\n",
      "        tqdm_args: dict = {},\n",
      "        max_line_width: int = 200,\n",
      "        seed: int = 42,\n",
      "    ) -> \"SrcDataset\":\n",
      "\n",
      "        srcs: dict[Path, tuple[str, Path]] = {\n",
      "            f: (f.read_text(), r)\n",
      "            for r in repos_paths\n",
      "            for f in sorted(r.glob(\"**/*.py\"))\n",
      "            if not f.is_symlink()\n",
      "        }\n",
      "        num_all_srcs = len(srcs)\n",
      "\n",
      "        def file_width(text):\n",
      "            return max(len(l) for l in text.split(\"\\n\"))\n",
      "\n",
      "        srcs = {\n",
      "            f: (code, r)\n",
      "            for f, (code, r) in srcs.items()\n",
      "            if file_width(code) <= max_line_width\n",
      "        }\n",
      "        result = SrcDataset(repos_root)\n",
      "        result.add_stats(\n",
      "            {\n",
      "                \"n_files_too_wide\": num_all_srcs - len(srcs),\n",
      "                \"too_wide_ratio\": (1 - len(srcs) / num_all_srcs),\n",
      "            }\n",
      "        )\n",
      "        masked_srcs: list[dict] = process_map(\n",
      "            mask_type_annots,\n",
      "            [(f, code[0]) for f, code in srcs.items()],\n",
      "            max_workers=max_workers,\n",
      "            desc=\"mask_type_annots\",\n",
      "            chunksize=max(1, len(srcs) // (8 * max_workers)),\n",
      "            **tqdm_args,\n",
      "        )\n",
      "        filtered_srcs = []\n",
      "\n",
      "        srcs_list = list(srcs.items())\n",
      "\n",
      "        rands = random.getstate()\n",
      "        random.seed(seed)\n",
      "        for i, x in enumerate(masked_srcs):\n",
      "            if x is None:\n",
      "                continue\n",
      "            n = len(x[\"types\"])\n",
      "            x[\"is_label\"] = [random.random() < label_ratio for _ in range(n)]\n",
      "            x[\"file\"] = srcs_list[i][0].relative_to(repos_root)\n",
      "            x[\"repo\"] = srcs_list[i][1][1].relative_to(repos_root)\n",
      "            filtered_srcs.append(x)\n",
      "        random.setstate(rands)\n",
      "\n",
      "        helper = _TokenizedSrcHelper(tokenizer)\n",
      "        tk_srcs: list[TokenizedSrc] = process_map(\n",
      "            helper.dict_to_tokenized_src,\n",
      "            filtered_srcs,\n",
      "            max_workers=max_workers,\n",
      "            desc=\"dict_to_tokenized_src\",\n",
      "            chunksize=max(1, len(filtered_srcs) // (8 * max_workers)),\n",
      "            **tqdm_args,\n",
      "        )\n",
      "\n",
      "        for f, g in groupby(tk_srcs, lambda s: s.file).items():\n",
      "            assert len(g) == 1, f\"{f} appears {len(g)} times.\"\n",
      "\n",
      "        result.all_srcs = tk_srcs\n",
      "        return result\n",
      "\n",
      "\n",
      "def type_check_src(\n",
      "    src: TokenizedSrc,\n",
      "    preds: dict[int, str],\n",
      "    mypy_path: Optional[Path] = None,\n",
      "    cwd: Optional[Path] = None,\n",
      ") -> tuple[dict[CodePosition, str] | str, str]:\n",
      "\n",
      "    code = src.origin_code\n",
      "    changes = list[tuple[CodeRange, int, str]]()\n",
      "    for i, pred in preds.items():\n",
      "        range = not_none(src.types_info[i].annot_range)\n",
      "        changes.append((range, 1, pred))\n",
      "    new_code = replace_strs_by_pos(code, changes)\n",
      "    check_r = MypyChecker.check_code(new_code, cwd=cwd, mypy_path=mypy_path)\n",
      "    feedback: dict[CodePosition, str] | str\n",
      "    if isinstance(check_r, str):\n",
      "        feedback = check_r\n",
      "    elif len(check_r.error_dict) == 0:\n",
      "        feedback = dict()\n",
      "    else:\n",
      "        assert len(check_r.error_dict) == 1\n",
      "        feedback = dict(list(check_r.error_dict.values())[0])\n",
      "    return feedback, new_code\n",
      "\n",
      "\n",
      "def mask_type_annots(\n",
      "    file_code: Union[str, tuple[Path, str]], silent: bool = True\n",
      ") -> Optional[dict]:\n",
      "\n",
      "    if isinstance(file_code, tuple):\n",
      "        src_path, code = file_code\n",
      "    else:\n",
      "        assert isinstance(file_code, str)\n",
      "        src_path = Path(\"[no source file]\")\n",
      "        code = file_code\n",
      "    try:\n",
      "        m = cst.parse_module(code)\n",
      "    except cst.ParserSyntaxError as e:\n",
      "        if not silent:\n",
      "            logging.warning(f\"Failed to parse src file: `{src_path}`\")\n",
      "        return None\n",
      "\n",
      "    annots_info, types = collect_user_annotations(m)\n",
      "    origin_code = m.code\n",
      "    types_str = [\n",
      "        m.code_for_node(not_none(info.annot).annotation) for info in annots_info\n",
      "    ]\n",
      "    mask_annot = cst.Annotation(cst.Name(SpecialNames.TypeMask))\n",
      "    replaces = dict()\n",
      "    for info in annots_info:\n",
      "        replaces[info.path] = mask_annot\n",
      "    new_code = apply_annotations(m, replaces).code\n",
      "    code_segs = new_code.split(SpecialNames.TypeMask)\n",
      "\n",
      "    assert (\n",
      "        len(code_segs) == len(types) + 1\n",
      "    ), f\"{len(code_segs)} != {len(types) + 1}. replaces: {replaces}\\ncode: {new_code}\"\n",
      "    return {\n",
      "        \"code_segs\": code_segs,\n",
      "        \"types\": types,\n",
      "        \"types_str\": types_str,\n",
      "        \"annots_info\": annots_info,\n",
      "        \"original_code\": origin_code,\n",
      "    }\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class CtxArgs:\n",
      "    ctx_size: int\n",
      "    ctx_margin: int\n",
      "    types_in_ctx: bool = False  \n",
      "\n",
      "    @property\n",
      "    def window_size(self) -> int:\n",
      "        return self.ctx_size - self.ctx_margin * 2\n",
      "\n",
      "\n",
      "def _tokenize_masked_code(\n",
      "    src: dict, src_id: int, tokenizer: TokenizerSPOT\n",
      ") -> list[int | tuple]:\n",
      "    bos_id = tokenizer.bos_token_id\n",
      "    eos_id = tokenizer.eos_token_id\n",
      "    assert bos_id is not None\n",
      "    assert eos_id is not None\n",
      "\n",
      "    all_tks: list[int | tuple] = []\n",
      "    segs: list[str] = src[\"code_segs\"]\n",
      "    types_labels: list[PythonType] = src[\"types\"]\n",
      "    types_info: list[AnnotInfo] = src[\"annots_info\"]\n",
      "\n",
      "    def as_tuple(p: CodePosition):\n",
      "        return (p.line, p.column)\n",
      "\n",
      "    labels_pos = [as_tuple(not_none(info.annot_range).start) for info in types_info]\n",
      "\n",
      "    if not issorted(labels_pos):\n",
      "        info_str = \"\\n\".join(map(str, types_info))\n",
      "        raise RuntimeError(\n",
      "            \"labels are not sorted according to their src locations:\\n\" + info_str\n",
      "        )\n",
      "\n",
      "    assert (\n",
      "        len(segs) == len(types_labels) + 1\n",
      "    ), f\"len(segs)={len(segs)}, len(types_labels)={len(types_labels)}\"\n",
      "    all_tks.append(bos_id)\n",
      "    for i in range(len(types_labels)):\n",
      "        all_tks.extend(tokenizer.encode(segs[i], add_special_tokens=False))\n",
      "        ty = types_labels[i]\n",
      "        ty_tks = tokenizer.encode(str(ty), add_special_tokens=False)\n",
      "        all_tks.append((ty, types_info[i], ty_tks, src_id))\n",
      "    all_tks.extend(tokenizer.encode(segs[-1], add_special_tokens=False))\n",
      "    all_tks.append(eos_id)\n",
      "    return all_tks\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class _ChunkingHelper:\n",
      "\n",
      "    tokenizer: TokenizerSPOT\n",
      "    ctx_args: CtxArgs\n",
      "\n",
      "    def tokenize(self, src: tuple[int, dict]):\n",
      "        return _tokenize_masked_code(src[1], src[0], self.tokenizer)\n",
      "\n",
      "    def process_chunk(self, tks: list[int | tuple]):\n",
      "        args = self.ctx_args\n",
      "        tokenizer = self.tokenizer\n",
      "\n",
      "        def expand_types_as_tks(mixed_tks: list):\n",
      "            result = list[int]()\n",
      "            mask_id = not_none(tokenizer.mask_token_id)\n",
      "            for e in mixed_tks:\n",
      "                if isinstance(e, int):\n",
      "                    result.append(e)\n",
      "                else:\n",
      "                    if args.types_in_ctx:\n",
      "                        assert isinstance(e[2], list)\n",
      "                        result.extend(e[2])\n",
      "                    else:\n",
      "                        result.append(mask_id)\n",
      "            return result\n",
      "\n",
      "        chunk_size, ctx_margin = args.ctx_size, args.ctx_margin\n",
      "        stride = chunk_size - 2 * ctx_margin\n",
      "        if len(tks) != chunk_size:\n",
      "            tks.extend([not_none(tokenizer.pad_token_id)] * (chunk_size - len(tks)))\n",
      "        extra_id = 0\n",
      "        middle = []\n",
      "        types = list[PythonType]()\n",
      "        types_tks = list[list[int]]()\n",
      "        annots_info = list[AnnotInfo]()\n",
      "        src_ids = list[int]()\n",
      "\n",
      "        for tk in tks[ctx_margin : ctx_margin + stride]:\n",
      "            if isinstance(tk, int):\n",
      "                middle.append(tk)\n",
      "            else:\n",
      "                ty, info, type_tks, src_id = tk\n",
      "                assert extra_id <= 99, \"> 99 annotations in a single sequence\"\n",
      "                middle.append(tokenizer.additional_special_tokens_ids[99 - extra_id])\n",
      "                types.append(ty)\n",
      "                types_tks.append(type_tks)\n",
      "                annots_info.append(info)\n",
      "                src_ids.append(src_id)\n",
      "                extra_id += 1\n",
      "        if extra_id == 0:\n",
      "            return None  \n",
      "        left_ctx = expand_types_as_tks(tks[:ctx_margin])[-ctx_margin:]\n",
      "        assert len(left_ctx) == ctx_margin, f\"{len(left_ctx)} != {ctx_margin}\"\n",
      "        right_ctx = expand_types_as_tks(tks[-ctx_margin:])[:ctx_margin]\n",
      "        assert len(right_ctx) == ctx_margin, f\"{len(right_ctx)} != {ctx_margin}\"\n",
      "        input_ids = left_ctx + middle + right_ctx\n",
      "        assert len(input_ids) == chunk_size\n",
      "\n",
      "        label_ids = [tokenizer.bos_token_id]\n",
      "        for i, type_tks in enumerate(types_tks):\n",
      "            label_ids.append(tokenizer.additional_special_tokens_ids[99 - i])\n",
      "            label_ids.extend(type_tks)\n",
      "        label_ids.append(tokenizer.eos_token_id)\n",
      "        meta = SrcChunkInfo(types, annots_info, src_ids)\n",
      "\n",
      "        return {\n",
      "            \"input_ids\": input_ids,\n",
      "            \"labels\": label_ids,\n",
      "            \"meta\": meta,\n",
      "        }\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class SrcChunkInfo:\n",
      "\n",
      "    types: list[PythonType]  \n",
      "    annots_info: list[AnnotInfo]  \n",
      "    src_ids: list[int]\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"SrcChunkInfo(num_types={len(self.types)}, unique_src_ids={set(self.src_ids)})\"\n",
      "\n",
      "\n",
      "@dataclass\n",
      "class ChunkedDataset:\n",
      "    data: Dataset\n",
      "    chunks_info: list[SrcChunkInfo]\n",
      "    files: list[Path]\n",
      "    file2src: dict[Path, str]\n",
      "    file2repo: dict[Path, Path]\n",
      "\n",
      "    def __getitem__(self, key: slice) -> \"ChunkedDataset\":\n",
      "        assert isinstance(key, slice)\n",
      "\n",
      "        new_data = {n: self.data[n][key] for n in self.data.column_names}\n",
      "        new_info = self.chunks_info[key]\n",
      "\n",
      "        return ChunkedDataset(\n",
      "            Dataset.from_dict(new_data),\n",
      "            chunks_info=new_info,\n",
      "            files=self.files,\n",
      "            file2src=self.file2src,\n",
      "            file2repo=self.file2repo,\n",
      "        )\n",
      "\n",
      "    def __repr__(self):\n",
      "        return f\"ChunkedDataset(num_chunks={len(self.chunks_info)}, num_srcs={len(self.files)})\"\n",
      "\n",
      "    def verify_labels(self, srcs: SrcDataset, tokenizer: TokenizerSPOT):\n",
      "\n",
      "        src_path_map = dict[Path, dict[AnnotPath, PythonType]]()\n",
      "        for f, src in srcs.file2src().items():\n",
      "            src_path_map[f] = {\n",
      "                info.path: ty for ty, info in zip(src.types, src.types_info)\n",
      "            }\n",
      "            assert_eq(len(src_path_map[f]), len(src.types))\n",
      "        for input, chunk in zip(self.data[\"input_ids\"], self.chunks_info):\n",
      "            for info, ty, sid in zip(chunk.annots_info, chunk.types, chunk.src_ids):\n",
      "                file = self.files[sid]\n",
      "                assert file in src_path_map, f\"{file} not in file2src.\"\n",
      "                assert (\n",
      "                    info.path in src_path_map[file]\n",
      "                ), f\"{info.path} should not be a label in {file}. Chunk code:\\n{tokenizer.decode(input)}\"\n",
      "                assert_eq(src_path_map[file][info.path], ty)\n",
      "\n",
      "\n",
      "def save_datasets(\n",
      "    datasets: dict[str, ChunkedDataset],\n",
      "    repos_split: dict[str, list[GitRepo]],\n",
      "    datasets_dir: Path,\n",
      "):\n",
      "    if datasets_dir.exists():\n",
      "        print(\"Deleting old datasets at:\", datasets_dir)\n",
      "        shutil.rmtree(datasets_dir)\n",
      "    datasets_dir.mkdir(parents=True)\n",
      "\n",
      "    with open(datasets_dir / \"repos_split.pkl\", \"wb\") as f:\n",
      "        pickle.dump(repos_split, f)\n",
      "\n",
      "    for name, dataset in datasets.items():\n",
      "        dataset.data.save_to_disk(str(datasets_dir / name))\n",
      "        extra = dataset.chunks_info, dataset.files, dataset.file2src, dataset.file2repo\n",
      "        with open(datasets_dir / f\"{name}-extra.pkl\", \"wb\") as f:\n",
      "            pickle.dump(extra, f)\n",
      "    import subprocess\n",
      "\n",
      "    subprocess.run([\"du\", \"-sh\", datasets_dir])\n",
      "\n",
      "\n",
      "def load_datasets(datasets_dir: Path):\n",
      "    set_names = [\"train\", \"valid\", \"test\"]\n",
      "    with open(datasets_dir / \"repos_split.pkl\", \"rb\") as f:\n",
      "        repos_split: dict[str, list[GitRepo]] = pickle.load(f)\n",
      "    datasets = dict[str, ChunkedDataset]()\n",
      "    for name in set_names:\n",
      "        with open(datasets_dir / f\"{name}-extra.pkl\", \"rb\") as f:\n",
      "            extra = pickle.load(f)\n",
      "        dataset = Dataset.load_from_disk(str(datasets_dir / name))\n",
      "        datasets[name] = ChunkedDataset(dataset, *extra)\n",
      "\n",
      "    return datasets, repos_split\n",
      "\n",
      "\n",
      "def output_ids_as_seqs(output_ids: Iterable[int], tokenizer: TokenizerSPOT):\n",
      "    seq_id = 0\n",
      "    buff = list[int]()\n",
      "    seqs = list[list[int]]()\n",
      "    mark = tokenizer.additional_special_tokens_ids[99 - seq_id]\n",
      "\n",
      "    for tk in output_ids:\n",
      "        if tk <= 0:\n",
      "            continue  \n",
      "        if tk != mark:\n",
      "            buff.append(tk)\n",
      "        else:\n",
      "            seqs.append(buff)\n",
      "            buff = []\n",
      "            seq_id += 1\n",
      "            mark = tokenizer.additional_special_tokens_ids[99 - seq_id]\n",
      "    seqs.append(buff)\n",
      "    return seqs[1:]\n",
      "\n",
      "\n",
      "def output_ids_as_types(\n",
      "    output_ids: Iterable[int], tokenizer: TokenizerSPOT, n_types: int\n",
      ") -> list[PythonType]:\n",
      "    seqs = output_ids_as_seqs(output_ids, tokenizer)\n",
      "    types = list[PythonType]()\n",
      "    for seq in seqs[:n_types]:\n",
      "        try:\n",
      "            ex_str = tokenizer.decode(seq, skip_special_tokens=True)\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Failed to decode sequence: {seq}\") from e\n",
      "        try:\n",
      "            tree = ast.parse(ex_str, mode=\"eval\").body\n",
      "            ty = parse_type_from_ast(tree)\n",
      "        except:\n",
      "            ty = PythonType.Any()\n",
      "        assert (\n",
      "            ty.__class__.__name__ == PythonType.__name__\n",
      "        ), f\"{ty} of type {type(ty)} is not a PythonType.\"\n",
      "        types.append(ty)\n",
      "    types.extend(PythonType.Any() for _ in range(n_types - len(types)))\n",
      "    assert len(types) == n_types\n",
      "    return types\n",
      "\n",
      "\n",
      "def patch_code_with_extra(\n",
      "    code: str, predictions: dict[CodeRange, str], errors: dict[CodePosition, str]\n",
      ") -> str:\n",
      "    replaces = []\n",
      "    for r, t in predictions.items():\n",
      "        replaces.append((r, 1, SpecialNames.TypeMask))\n",
      "        replaces.append((CodeRange(r.start, r.start), 2, f\"/* {t} */\"))\n",
      "\n",
      "    for p, e in errors.items():\n",
      "        replaces.append((CodeRange(p, p), 3, f\"/* error: {e} */\"))\n",
      "\n",
      "    return replace_strs_by_pos(code, replaces)\n",
      "\n",
      "\n",
      "def compute_metrics(\n",
      "    predictions: np.ndarray,\n",
      "    label_ids: np.ndarray,\n",
      "    cats: list[AnnotCat],\n",
      "    n_labels: Sequence[int],\n",
      "    tokenizer: TokenizerSPOT,\n",
      ") -> dict[str, Any]:\n",
      "    assert len(predictions.shape) == 2\n",
      "    assert (n_rows := predictions.shape[0]) == label_ids.shape[0]\n",
      "    preds = list[PythonType]()\n",
      "    labels = list[PythonType]()\n",
      "    for i in tqdm(range(n_rows), desc=\"decoding types\"):\n",
      "        pred = output_ids_as_types(predictions[i, :], tokenizer, n_labels[i])\n",
      "        label = output_ids_as_types(label_ids[i, :], tokenizer, n_labels[i])\n",
      "        preds.extend(map(normalize_type, pred))\n",
      "        labels.extend(map(normalize_type, label))\n",
      "\n",
      "    r = type_accuracies(preds, labels, cats, normalize_types=False)\n",
      "    r[\"pred_types\"] = [ty.head_name() for ty in preds]\n",
      "    r[\"label_types\"] = [ty.head_name() for ty in labels]\n",
      "    return r\n",
      "\n",
      "\n",
      "def type_accuracies(\n",
      "    pred_types: Sequence[PythonType],\n",
      "    label_types: Sequence[PythonType],\n",
      "    types_cat: Sequence[AnnotCat],\n",
      "    normalize_types=True,\n",
      ") -> dict[str, Any]:\n",
      "    assert len(pred_types) == len(\n",
      "        label_types\n",
      "    ), f\"{len(pred_types)} != {len(label_types)}\"\n",
      "\n",
      "    def safe_div(a, b):\n",
      "        if b == 0:\n",
      "            return float(\"nan\")\n",
      "        return a / b\n",
      "\n",
      "    if normalize_types:\n",
      "        pred_types = [normalize_type(ty) for ty in pred_types]\n",
      "        label_types = [normalize_type(ty) for ty in label_types]\n",
      "\n",
      "    n_correct_by_cat = Counter[AnnotCat]()\n",
      "    n_partial_by_cat = Counter[AnnotCat]()\n",
      "    n_label_by_cat = Counter[AnnotCat](types_cat)\n",
      "    n_partial_no_any = 0\n",
      "    n_label_no_any = 0\n",
      "\n",
      "    for p, l, cat in zip(pred_types, label_types, types_cat):\n",
      "        if p == l:\n",
      "            n_correct_by_cat[cat] += 1\n",
      "        if p.head_name() == l.head_name():\n",
      "            n_partial_by_cat[cat] += 1\n",
      "        if l.head_name() != \"Any\":\n",
      "            n_label_no_any += 1\n",
      "            if p.head_name() == l.head_name():\n",
      "                n_partial_no_any += 1\n",
      "\n",
      "    partial_acc = safe_div(n_partial_by_cat.total(), n_label_by_cat.total())\n",
      "    partial_accs = {}\n",
      "    for k in sorted(n_partial_by_cat.keys(), key=lambda k: k.value):\n",
      "        partial_accs[k.name] = safe_div(n_partial_by_cat[k], n_label_by_cat[k])\n",
      "\n",
      "    full_acc = safe_div(n_correct_by_cat.total(), n_label_by_cat.total())\n",
      "    full_accs = {}\n",
      "    for k in sorted(n_correct_by_cat.keys(), key=lambda k: k.value):\n",
      "        full_accs[k.name] = safe_div(n_correct_by_cat[k], n_label_by_cat[k])\n",
      "\n",
      "    return {\n",
      "        \"partial_acc\": partial_acc,\n",
      "        \"partial_acc_wo_any\": safe_div(n_partial_no_any, n_label_no_any),\n",
      "        \"partial_accs\": partial_accs,\n",
      "        \"full_acc\": full_acc,\n",
      "        \"full_accs\": full_accs,\n",
      "        \"n_labels\": n_label_by_cat.total(),\n",
      "    }\n",
      "\n",
      "\n",
      "def pretty_print_dict(\n",
      "    d: dict,\n",
      "    level: int = 0,\n",
      "    max_show_level: int = 1000,\n",
      "    float_precision: int = 5,\n",
      "):\n",
      "    for k, v in d.items():\n",
      "        print(\"   \" * level, end=\"\")\n",
      "        if isinstance(v, float):\n",
      "            print(f\"{k}: %.{float_precision}g\" % v)\n",
      "        elif isinstance(v, dict) or isinstance(v, list):\n",
      "            if level >= max_show_level:\n",
      "                print(f\"{k}: ...\")\n",
      "            else:\n",
      "                print(f\"{k}:\")\n",
      "                if isinstance(v, list):\n",
      "                    v = {f\"[{i}]\": e for i, e in enumerate(v)}\n",
      "                pretty_print_accuracies(\n",
      "                    v, level=level + 1, max_show_level=max_show_level\n",
      "                )\n",
      "        else:\n",
      "            print(f\"{k}: {v}\")\n",
      "\n",
      "\n",
      "def pretty_print_accuracies(\n",
      "    accs: dict[str, Any],\n",
      "    level: int = 0,\n",
      "    max_show_level: int = 1000,\n",
      "):\n",
      "    pretty_print_dict(\n",
      "        accs, level=level, max_show_level=max_show_level, float_precision=4\n",
      "    )\n",
      "\n",
      "\n",
      "def preds_to_accuracies(preds: Sequence[Sequence[PythonType]], dataset: ChunkedDataset):\n",
      "    cats = [an.cat for info in dataset.chunks_info for an in info.annots_info]\n",
      "    labels = [ty for info in dataset.chunks_info for ty in info.types]\n",
      "    return type_accuracies(list(seq_flatten(preds)), labels, cats)\n",
      "\n",
      "\n",
      "def _turn_off_tokenizer_warning(tokenizer: TokenizerSPOT):\n",
      "    tokenizer.deprecation_warnings[\n",
      "        \"sequence-length-is-longer-than-the-specified-maximum\"\n",
      "    ] = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cst.parse_module((proj_root() / \"src/spot/data.py\").read_text()).visit(CommentRemover()).code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from spot.data import GitRepo, ModuleRemapUnpickler\n",
    "from spot.type_env import (\n",
    "    AnnotPath,\n",
    "    MypyChecker,\n",
    "    SelectAnnotations,\n",
    "    TypeInfAction,\n",
    "    TypeInfEnv,\n",
    "    TypeInfState,\n",
    "    collect_annotations,\n",
    "    mypy_checker,\n",
    ")\n",
    "from spot.utils import cst, proj_root, read_file, seq_flatten, tqdm, write_file\n",
    "\n",
    "os.chdir(proj_root())\n",
    "\n",
    "datadir = Path(os.getenv(\"datadir\"))\n",
    "repos_dir = datadir / \"SPOT-data/repos\"\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "rename_module = lambda n: \"spot.data\" if n == \"spot.data_prepare\" else n\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    useful_repos: list[GitRepo] = ModuleRemapUnpickler(f, rename_module).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiayi/Projects/SPOT/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# loading pre-trained model and tokenizer\n",
    "\n",
    "model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-with_margin/\"\n",
    "# model_dir = datadir / \"checkpoints/saved/SPOT-CodeT5-no_margin/\"\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    RobertaTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers.models.t5 import T5ForConditionalGeneration\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer: RobertaTokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "model: T5ForConditionalGeneration = T5ForConditionalGeneration.from_pretrained(\n",
    "    model_dir\n",
    ").to(device)\n",
    "max_target_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "inference_dir = Path(\"data/code_output/inference\")\n",
    "if inference_dir.exists():\n",
    "    shutil.rmtree(inference_dir)\n",
    "inference_dir.mkdir(parents=True)\n",
    "write_file(inference_dir / \"env_code_1.py\", read_file(\"data/code/env_code_1.py\"))\n",
    "write_file(inference_dir / \"env_code_2.py\", read_file(\"data/code/env_code_2.py\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9077, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spot.data import mask_type_annots, output_ids_as_types, tokenize_masked\n",
    "\n",
    "test_code = \"\"\"\n",
    "@dataclass\n",
    "class GitRepo:\n",
    "    author: str\n",
    "    name: str\n",
    "    url: str\n",
    "    stars: int\n",
    "    forks: int\n",
    "\n",
    "    def authorname(self):\n",
    "        return self.author + \"__\" + self.name\n",
    "\n",
    "    def repo_dir(self, repos_dir: Path) -> Path:\n",
    "        return repos_dir / \"downloaded\" / self.authorname()\n",
    "\n",
    "    def download(self, repos_dir: Path, timeout=None) -> bool:\n",
    "        pass\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_model(code: str, num_beams=16):\n",
    "    masked = mask_type_annots((Path(\"no_source\"), code))\n",
    "    tks = tokenize_masked(masked, tokenizer, device)\n",
    "    input_ids = tks[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        loss = model.forward(**tks).loss\n",
    "        dec = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_target_length,\n",
    "            num_beams=num_beams,\n",
    "            # do_sample=True,\n",
    "        )[0]\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"labels\": output_ids_as_types(tks[\"labels\"][0], tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "        \"input_ids\": input_ids[0],\n",
    "        \"output_ids\": dec,\n",
    "        \"annots_info\": masked[\"annots_info\"],\n",
    "    }\n",
    "\n",
    "\n",
    "result = run_model(test_code, num_beams=10)\n",
    "result[\"loss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spot import PythonType\n",
    "from spot.type_env import apply_annotations\n",
    "\n",
    "\n",
    "def type_to_annot(ty: PythonType) -> str:\n",
    "    return cst.Annotation(cst.parse_expression(str(ty)))\n",
    "\n",
    "\n",
    "def run_aug_model(src: Path, cwd: Path):\n",
    "    result = run_model(read_file(src), num_beams=10)\n",
    "    pred_annots = {\n",
    "        info.path: type_to_annot(t)\n",
    "        for info, t in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    m1 = apply_annotations(cst.parse_module(read_file(src)), pred_annots)\n",
    "    write_file(src, m1.code)\n",
    "    checker_r = MypyChecker.check_project(src, cwd)\n",
    "    pos_to_preds = {\n",
    "        info.annot_range: str(ty)\n",
    "        for info, ty in zip(result[\"annots_info\"], result[\"predicted_types\"])\n",
    "    }\n",
    "    return {\n",
    "        \"model_result\": result,\n",
    "        \"module\": m1,\n",
    "        \"checker_feedback\": checker_r,\n",
    "        \"pos_to_preds\": pos_to_preds,\n",
    "    }\n",
    "\n",
    "\n",
    "aug_r = run_aug_model(inference_dir / \"env_code_2.py\", inference_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- model output ----\n",
      "<pad><s><extra_id_0>int<extra_id_1>int<extra_id_2>int<extra_id_3>int<extra_id_4>int, y : int<extra_id_5>int<extra_id_6>Optional[int]<extra_id_7>int<extra_id_8>int<extra_id_9>Bar[int, int, int, float, float]</s>\n",
      "---- checker_feedback ----\n",
      "env_code_2.py:20:14: error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  [assignment]\n",
      "env_code_2.py:32:29: error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  [arg-type]\n",
      "env_code_2.py:35:6: error: \"Bar\" expects no type arguments, but 5 given  [type-arg]\n",
      "Found 3 errors in 1 file (checked 1 source file)\n",
      "\n",
      "---- new input ----\n",
      "# Env example 2: some existing annotations\n",
      "\n",
      "from typing import *\n",
      "\n",
      "\n",
      "def fib(n: /* int */<extra_id_0>):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n - 1) + fib(n - 2)\n",
      "\n",
      "\n",
      "def foo(bar: /* int */<extra_id_1>):\n",
      "    return fib(bar)\n",
      "\n",
      "\n",
      "class Bar:\n",
      "    z: /* int */<extra_id_2> = /* error: Incompatible types in assignment (expression has type \"str\", variable has type \"int\")  */\"hello\"\n",
      "    w: /* int */<extra_id_3>\n",
      "\n",
      "    def __init__(self, x: /* Any */<extra_id_4>):\n",
      "        self.x: /* int */<extra_id_5> = x\n",
      "        self.y: /* Optional[int] */<extra_id_6> = None\n",
      "        self.reset(self.z)\n",
      "\n",
      "    def reset(self, w0):\n",
      "        self.w = w0\n",
      "\n",
      "    def foo(self, z: /* int */<extra_id_7>) -> /* int */<extra_id_8>:\n",
      "        return self.x + len(/* error: Argument 1 to \"len\" has incompatible type \"int\"; expected \"Sized\"  */z)\n",
      "\n",
      "\n",
      "bar: /* Bar[int, int, int, float, float] *//* error: \"Bar\" expects no type arguments, but 5 given  */<extra_id_9> = Bar(3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.utils import patch_code_with_extra\n",
    "print(\"---- predicted types ----\")\n",
    "print(aug_r['model_result']['predicted_types'])\n",
    "print(\"---- model output ----\")\n",
    "print(tokenizer.decode(aug_r[\"model_result\"][\"output_ids\"], skip_special_tokens=False))\n",
    "print(\"---- checker_feedback ----\")\n",
    "print(aug_r[\"checker_feedback\"].output_str)\n",
    "\n",
    "print(\"---- new input ----\")\n",
    "new_input = patch_code_with_extra(\n",
    "    aug_r[\"module\"].code,\n",
    "    aug_r[\"pos_to_preds\"],\n",
    "    aug_r[\"checker_feedback\"].error_dict[\"env_code_2.py\"],\n",
    ")\n",
    "print(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predicted_types': [int,\n",
       "  int,\n",
       "  int,\n",
       "  int,\n",
       "  int,\n",
       "  int,\n",
       "  None,\n",
       "  int,\n",
       "  int,\n",
       "  Bar[int, int, int, float, float]],\n",
       " 'generation': '<pad><s><extra_id_0>int<extra_id_1>int<extra_id_2>int<extra_id_3>int<extra_id_4>int<extra_id_5>int<extra_id_6>None<extra_id_7>int<extra_id_8>int<extra_id_9>Bar[int, int, int, float, float]</s>'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_model_with_extra(code: str, num_beams=16):\n",
    "    input_ids = tokenizer.encode(code, return_tensors=\"pt\").to(device)\n",
    "    dec = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_target_length,\n",
    "        num_beams=num_beams,\n",
    "    )[0]\n",
    "    return {\n",
    "        \"predicted_types\": output_ids_as_types(dec, tokenizer),\n",
    "        \"generation\": tokenizer.decode(dec),\n",
    "    }\n",
    "\n",
    "\n",
    "run_model_with_extra(new_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Env example 2: some existing annotations\n",
      "\n",
      "from typing import *\n",
      "\n",
      "\n",
      "def fib(n: int):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n - 1) + fib(n - 2)\n",
      "\n",
      "\n",
      "def foo(bar: int):\n",
      "    return fib(bar)\n",
      "\n",
      "\n",
      "class Bar:\n",
      "    z: int = /*[assignment]*/\"hello\"\n",
      "    w: int\n",
      "\n",
      "    def __init__(self, x: Any):\n",
      "        self.x: int = x\n",
      "        self.y: Optional[int] = None\n",
      "        self.reset(self.z)\n",
      "\n",
      "    def reset(self, w0):\n",
      "        self.w = w0\n",
      "\n",
      "    def foo(self, z: int) -> int:\n",
      "        return self.x + len(z)\n",
      "\n",
      "\n",
      "bar: Bar[int, int, int, float, float] = Bar(3)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spot.utils import insert_strings\n",
    "\n",
    "print(insert_strings(aug_r[\"module\"].code, [(19, 13, \"/*[assignment]*/\")]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "@dataclass\n",
      "class GitRepo:\n",
      "    author:<extra_id_0>\n",
      "    name:<extra_id_1>\n",
      "    url:<extra_id_2>\n",
      "    stars:<extra_id_3>\n",
      "    forks:<extra_id_4>\n",
      "\n",
      "    def authorname(self):\n",
      "        return self.author + \"__\" + self.name\n",
      "\n",
      "    def repo_dir(self, repos_dir:<extra_id_5>) -><extra_id_6>:\n",
      "        return repos_dir / \"downloaded\" / self.authorname()\n",
      "\n",
      "    def download(self, repos_dir:<extra_id_7>, timeout=None) -><extra_id_8>:\n",
      "        pass\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Replace all types to predict with special tokens\n",
    "print(tokenizer.decode(result[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '', '@', 'data', 'class', '', 'class', 'Git', 'Repo', ':', '', '', 'author', ':', '<extra_id_0>', '', '', 'name', ':', '<extra_id_1>', '', '', 'url', ':', '<extra_id_2>', '', '', 'stars', ':', '<extra_id_3>', '', '', 'for', 'ks', ':', '<extra_id_4>', '', '', '', 'def', 'author', 'name', '(', 'self', '):', '', '', 'return', 'self', '.', 'author', '+', '\"__', '\"', '+', 'self', '.', 'name', '', '', '', 'def', 'repo', '_', 'dir', '(', 'self', ',', 'repos', '_', 'dir', ':', '<extra_id_5>', ')', '->', '<extra_id_6>', ':', '', '', 'return', 'repos', '_', 'dir', '/', '\"', 'down', 'loaded', '\"', '/', 'self', '.', 'author', 'name', '()', '', '', '', 'def', 'download', '(', 'self', ',', 'repos', '_', 'dir', ':', '<extra_id_7>', ',', 'timeout', '=', 'None', ')', '->', '<extra_id_8>', ':', '', '', 'pass', '', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Tokenize using Byte Pair Encoding (BPE)\n",
    "print(tokenizer.convert_ids_to_tokens(result[\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<s>', '<extra_id_0>', 'str', '<extra_id_1>', 'str', '<extra_id_2>', 'str', '<extra_id_3>', 'List', '[', 'str', ']', '+', 'List', '[', 'str', ']', '<extra_id_4>', 'List', '[', 'str', ']', '+', 'List', '[', 'str', ']', '+', 'List', '[', 'str', ']', '<extra_id_5>', 'Path', '<extra_id_6>', 'Path', '.', 'Path', '<extra_id_7>', 'Path', '.', 'Path', '<extra_id_8>', 'Path', '.', 'Path', '[', 'str', ']', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Let model predict a sequence of types using BPE\n",
    "print(tokenizer.convert_ids_to_tokens(result[\"output_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[str, str, str, Any, Any, Path, Path.Path, Path.Path, Path.Path[str]]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Extract the predicted types\n",
    "print(result[\"predicted_types\"])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ffc72953da4dd16b2e00785be9c4013ef131f465a8658f3921b6634d4eeec8"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
